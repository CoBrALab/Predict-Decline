{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate new data, if needed\n",
    "ADNI_dir = \"/data/chamal/projects/charles/Predict-Decline/derivatives/ADNIMERGE/\"\n",
    "\n",
    "X_baseline = [] # baseline time point\n",
    "for filename in sorted(os.listdir(ADNI_dir + \"baseline/\")):\n",
    "    ct = np.loadtxt(ADNI_dir + \"baseline/\"+filename, delimiter='\\n')\n",
    "    X_baseline.append(ct)\n",
    "    \n",
    "X_followup = [] # other longitudinal time point\n",
    "for filename in sorted(os.listdir(ADNI_dir + \"followup/\")):\n",
    "    ct = np.loadtxt(ADNI_dir + \"followup/\"+filename, delimiter='\\n')\n",
    "    X_followup.append(ct)\n",
    "\n",
    "print(len(X_baseline), len(X_followup))\n",
    "np.save(ADNI_dir + \"X_baseline.npy\", np.array(X_baseline))\n",
    "np.save(ADNI_dir + \"X_followup.npy\", np.array(X_followup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the AIBL replication data\n",
    "AIBL_dir = \"/data/chamal/projects/charles/Predict-Decline/derivatives/AIBL/\"\n",
    "\n",
    "aibl_baseline = []\n",
    "for filename in sorted(os.listdir(AIBL_dir + \"baseline/\")):\n",
    "    ct = np.loadtxt(AIBL_dir + \"baseline/\"+filename, delimiter='\\n')\n",
    "    aibl_baseline.append(ct)\n",
    "    \n",
    "aibl_followup = [] # other longitudinal time point\n",
    "for filename in sorted(os.listdir(AIBL_dir + \"followup/\")):\n",
    "    ct = np.loadtxt(AIBL_dir + \"followup/\"+filename, delimiter='\\n')\n",
    "    aibl_followup.append(ct)\n",
    "\n",
    "print(len(aibl_baseline), len(aibl_followup))\n",
    "np.save(AIBL_dir + \"aibl_baseline.npy\", np.array(aibl_baseline))\n",
    "np.save(AIBL_dir + \"aibl_followup.npy\", np.array(aibl_followup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_dir = \"/data/chamal/projects/charles/Predict-Decline/derivatives/data_frames/\"\n",
    "ADNI_dir = \"/data/chamal/projects/charles/Predict-Decline/derivatives/ADNIMERGE/\"\n",
    "AIBL_dir = \"/data/chamal/projects/charles/Predict-Decline/derivatives/AIBL/\"\n",
    "\n",
    "df = pd.read_csv(\"Exp_502_602_combined.csv\")\n",
    "sub_list = df[df[\"STATUS\"].isin([\"OK\", \"Add\", \"New\"])]\n",
    "X_baseline = np.load(ADNI_dir + \"X_baseline.npy\")\n",
    "X_followup = np.load(ADNI_dir + \"X_followup.npy\")\n",
    "aibl_baseline = np.load(AIBL_dir + \"aibl_baseline.npy\")\n",
    "aibl_followup = np.load(AIBL_dir + \"aibl_followup.npy\")\n",
    "X_diff = X_followup - X_baseline # vertex-wise CT change\n",
    "n_subjects = sub_list.shape[0]\n",
    "print(n_subjects, X_baseline.shape, X_followup.shape, aibl_baseline.shape, aibl_followup.shape)\n",
    "\n",
    "ADNI_out = \"/data/chamal/projects/charles/Predict-Decline/derivatives/ADNIMERGE_reduced/\"\n",
    "AIBL_out = \"/data/chamal/projects/charles/Predict-Decline/derivatives/AIBL_reduced/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the train-test splits (old script)\n",
    "splits_dir = \"/data/chamal/projects/charles/Predict-Decline/methods/splits/\"\n",
    "\n",
    "frac_train = 0.8\n",
    "ncv = 10\n",
    "splits = {\"train\": [], \"test\": []}\n",
    "\n",
    "for i in range(ncv):\n",
    "    indices = list(range(n_subjects))\n",
    "    random.shuffle(indices)\n",
    "    train_split = indices[:int(0.8*len(indices))]\n",
    "    valid_split = indices[int(0.8*len(indices)):]\n",
    "    splits[\"train\"].append(train_split)\n",
    "    splits[\"test\"].append(valid_split)\n",
    "    \n",
    "splits = pd.DataFrame.from_dict(splits)\n",
    "splits.to_pickle(splits_dir + \"train_test_splits.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the train-test splits (modified)\n",
    "# Splits balanced with respect to trajectory classes and subgroups\n",
    "\n",
    "ncv = 10\n",
    "frac_test = 1/ncv\n",
    "predefined_splits = {'MMSE': list(range(ncv)), 'ADAS13': list(range(ncv))}\n",
    "splits = {'MMSE': {'train': [], 'test': []},\n",
    "          'ADAS13': {'train': [], 'test': []}}\n",
    "indices = {'MMSE': {'T1': {'BE': [], 'FE': [], 'CC': []},\n",
    "                    'T2': {'BE': [], 'FE': [], 'CC': []}},\n",
    "           'ADAS13': {'T1': {'BE': [], 'FE': [], 'CC': []},\n",
    "                      'T2': {'BE': [], 'FE': [], 'CC': []},\n",
    "                      'T3': {'BE': [], 'FE': [], 'CC': []}}}\n",
    "\n",
    "sub_list = sub_list.reset_index(drop=True)\n",
    "\n",
    "# find the trajectory and subgroup of each index\n",
    "for index, row in sub_list.iterrows():\n",
    "    mmse_traj = row['MMSE_2c_traj']\n",
    "    mmse_group = row['MMSE_gr']\n",
    "    indices['MMSE']['T{}'.format(mmse_traj+1)][mmse_group].append(index)\n",
    "    \n",
    "    adas_traj = row['ADAS_3c_traj']\n",
    "    adas_group = row['ADAS13_gr']\n",
    "    indices['ADAS13']['T{}'.format(adas_traj+1)][adas_group].append(index)\n",
    "    \n",
    "# partition into balanced train and test splits\n",
    "for i in indices.keys():\n",
    "    train_split = [[] for cv_iter in range(ncv)]\n",
    "    test_split = [[] for cv_iter in range(ncv)]\n",
    "    \n",
    "    for j in indices[i].keys():\n",
    "        for k in indices[i][j].keys():\n",
    "            random.shuffle(indices[i][j][k])\n",
    "            length = len(indices[i][j][k])\n",
    "            \n",
    "            for cv_iter in range(ncv):\n",
    "                start = int(length*cv_iter*frac_test)\n",
    "                end = int(length*(cv_iter+1)*frac_test)\n",
    "                test_split[cv_iter].extend(indices[i][j][k][start:end])\n",
    "                before = indices[i][j][k][:start]\n",
    "                after = indices[i][j][k][end:]\n",
    "                train_split[cv_iter].extend(before + after)\n",
    "                \n",
    "    for cv_iter in range(ncv):\n",
    "        random.shuffle(train_split[cv_iter])\n",
    "        random.shuffle(test_split[cv_iter])\n",
    "        \n",
    "        # predefined splits for grid search\n",
    "        predefined_split = list(range(ncv))*int((len(train_split[cv_iter])/ncv))\n",
    "        predefined_split += list(range(len(train_split[cv_iter])%ncv))\n",
    "        random.shuffle(predefined_split)\n",
    "        predefined_splits[i][cv_iter] = predefined_split\n",
    "    \n",
    "    splits[i]['train'] = train_split\n",
    "    splits[i]['test'] = test_split\n",
    "        \n",
    "splits = pd.DataFrame.from_dict(splits)\n",
    "splits.to_pickle(splits_dir + \"train_test_splits.pkl\")\n",
    "predefined_splits = pd.DataFrame.from_dict(predefined_splits)\n",
    "predefined_splits.to_pickle(splits_dir + \"predefined_splits.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reduce the data with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "ncv = 10\n",
    "n_components = 78 # same number of components as AAL\n",
    "splits = pd.read_pickle(splits_dir + \"train_test_splits.pkl\")\n",
    "\n",
    "for t in ['MMSE', 'ADAS13']:\n",
    "    for i in range(ncv):\n",
    "        print(\"Reducing data, {} trajectory, fold {}\".format(t, i))\n",
    "        train_split = splits[t]['train'][i]\n",
    "        test_split = splits[t]['test'][i]\n",
    "        X_train_baseline = X_baseline[train_split]\n",
    "        X_test_baseline = X_baseline[test_split]\n",
    "        X_train_followup = X_followup[train_split]\n",
    "        X_test_followup = X_followup[test_split]\n",
    "        X_train_diff = X_diff[train_split]\n",
    "\n",
    "        tstart = time.time()\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X_train_diff)\n",
    "        X_bl_train_reduced = pca.transform(X_train_baseline)\n",
    "        X_bl_test_reduced = pca.transform(X_test_baseline)\n",
    "        X_vartp_train_reduced = pca.transform(X_train_followup)\n",
    "        X_vartp_test_reduced = pca.transform(X_test_followup)\n",
    "        print(\"Time required : {}\".format(time.time() - tstart))\n",
    "        np.save(ADNI_out + \"PCA_bl_train_{}_cv{}.npy\".format(t,i), X_bl_train_reduced)\n",
    "        np.save(ADNI_out + \"PCA_bl_test_{}_cv{}.npy\".format(t,i), X_bl_test_reduced)\n",
    "        np.save(ADNI_out + \"PCA_vartp_train_{}_cv{}.npy\".format(t,i), X_vartp_train_reduced)\n",
    "        np.save(ADNI_out + \"PCA_vartp_test_{}_cv{}.npy\".format(t,i), X_vartp_test_reduced)\n",
    "        \n",
    "        if t == 'MMSE': # Reduce the AIBL replication data\n",
    "            aibl_bl_reduced = pca.transform(aibl_baseline)\n",
    "            aibl_vartp_reduced = pca.transform(aibl_followup)\n",
    "            np.save(AIBL_out + \"PCA_bl_cv{}.npy\".format(i), aibl_bl_reduced)\n",
    "            np.save(AIBL_out + \"PCA_vartp_cv{}.npy\".format(i), aibl_vartp_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the data with RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "n_features = 78 # same number of components as AAL\n",
    "splits = pd.read_pickle(splits_dir + \"train_test_splits.pkl\")\n",
    "\n",
    "for t in ['MMSE', 'ADAS13']:\n",
    "    for i in range(ncv):\n",
    "        print(\"Reducing data, {} trajectory, fold {}\".format(t, i))\n",
    "        train_split = splits[t]['train'][i]\n",
    "        test_split = splits[t]['test'][i]\n",
    "        X_train_baseline = X_baseline[train_split]\n",
    "        X_test_baseline = X_baseline[test_split]\n",
    "        X_train_followup = X_followup[train_split]\n",
    "        X_test_followup = X_followup[test_split]\n",
    "        X_train_diff = X_diff[train_split]\n",
    "\n",
    "        df_train = sub_list.iloc[train_split]\n",
    "        if t == 'MMSE':\n",
    "            y_train = df_train[\"MMSE_2c_traj\"].values\n",
    "        elif t == 'ADAS13':\n",
    "            y_train = df_train[\"ADAS_3c_traj\"].values\n",
    "\n",
    "        tstart = time.time()\n",
    "        estimator = SVC(kernel='linear')\n",
    "        rfe = RFE(estimator, n_features, step=0.5)\n",
    "        rfe.fit(X_train_diff, y_train)\n",
    "        X_bl_train = rfe.transform(X_train_baseline)\n",
    "        X_bl_test = rfe.transform(X_test_baseline)\n",
    "        X_vartp_train = rfe.transform(X_train_followup)\n",
    "        X_vartp_test = rfe.transform(X_test_followup)\n",
    "        print(\"Time required : {}\".format(time.time() - tstart))\n",
    "        np.save(ADNI_out + \"RFE_bl_train_{}_cv{}.npy\".format(t,i), X_bl_train)\n",
    "        np.save(ADNI_out + \"RFE_bl_test_{}_cv{}.npy\".format(t,i), X_bl_test)\n",
    "        np.save(ADNI_out + \"data/RFE_vartp_train_{}_cv{}.npy\".format(t,i), X_vartp_train)\n",
    "        np.save(ADNI_out + \"data/RFE_vartp_test_{}_cv{}.npy\".format(t,i), X_vartp_test)\n",
    "        \n",
    "        if t == 'MMSE': # Reduce the AIBL replication data\n",
    "            aibl_bl_reduced = rfe.transform(aibl_baseline)\n",
    "            aibl_vartp_reduced = rfe.transform(aibl_followup)\n",
    "            np.save(AIBL_out + \"data_replication/RFE_bl_cv{}.npy\".format(i), aibl_bl_reduced)\n",
    "            np.save(AIBL_out + \"data_replication/RFE_vartp_cv{}.npy\".format(i), aibl_vartp_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the data with RLR\n",
    "# Idea by Moradi et al., 2015\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "n_features = 78 # same number of components as AAL\n",
    "splits = pd.read_pickle(splits_dir + \"train_test_splits.pkl\")\n",
    "\n",
    "n_repeats = 5 # 10 by Moradi et al.\n",
    "range_size = 10 # 100 by Moradi et al.\n",
    "alpha_range = np.power(10, np.linspace(-5, -2, range_size))\n",
    "print(\"Range of alphas: {}\".format(alpha_range))\n",
    "\n",
    "for t in ['MMSE', 'ADAS13']:\n",
    "    for i in range(ncv):\n",
    "        train_split = splits[t][\"train\"][i]\n",
    "        test_split = splits[t][\"test\"][i]\n",
    "        X_train_baseline = X_baseline[train_split]\n",
    "        X_test_baseline = X_baseline[test_split]\n",
    "        X_train_followup = X_followup[train_split]\n",
    "        X_test_followup = X_followup[test_split]\n",
    "        X_train_diff = X_diff[train_split]\n",
    "\n",
    "        df_train = sub_list.iloc[train_split]\n",
    "        if t == 'MMSE':\n",
    "            y_train = df_train[\"MMSE_2c_traj\"]\n",
    "        elif t == 'ADAS13':\n",
    "            y_train = df_train[\"ADAS_3c_traj\"]\n",
    "\n",
    "        print(\"Begin {}, CV fold number {}\".format(t,i))\n",
    "        print(\"Step one: choose the optimal alpha\")\n",
    "        tstart = time.time()\n",
    "        alpha_star = []\n",
    "\n",
    "        for j in range(n_repeats):\n",
    "            print(\"Beginning repeat {}...\".format(j))\n",
    "            scores = np.zeros(range_size)\n",
    "\n",
    "            indices = list(range(len(train_split)))\n",
    "            random.shuffle(indices)\n",
    "            inner_train = indices[:int(0.9*len(indices))]\n",
    "            inner_valid = indices[int(0.9*len(indices)):]\n",
    "            X_diff_inner_train = X_diff[inner_train]\n",
    "            X_diff_inner_valid = X_diff[inner_valid]\n",
    "            df_inner_train = df_train.iloc[inner_train]\n",
    "            df_inner_valid = df_train.iloc[inner_valid]\n",
    "            if t == 'MMSE':\n",
    "                y_inner_train = df_inner_train[\"MMSE_2c_traj\"]\n",
    "                y_inner_valid = df_inner_valid[\"MMSE_2c_traj\"]\n",
    "            elif t == 'ADAS13':\n",
    "                y_inner_train = df_inner_train[\"ADAS_3c_traj\"]\n",
    "                y_inner_valid = df_inner_valid[\"ADAS_3c_traj\"]\n",
    "\n",
    "            for k in range(range_size):\n",
    "                clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", l1_ratio=0.5, alpha=alpha_range[k])\n",
    "                clf.fit(X_diff_inner_train, y_inner_train)\n",
    "                scores[k] = clf.score(X_diff_inner_valid, y_inner_valid)\n",
    "\n",
    "            alpha_star.append(alpha_range[np.argmax(scores)])\n",
    "\n",
    "        print(\"Optimal alpha: {}\".format(np.median(alpha_star)))\n",
    "        print(\"Step two: find the most significant features\")\n",
    "        k = np.where(alpha_range <= np.median(alpha_star))[0][-1]\n",
    "        nonzero_features = np.ones(X_baseline.shape[1])\n",
    "\n",
    "        for alpha in alpha_range[k:]:\n",
    "            clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\",\n",
    "                                l1_ratio=0.5, alpha=alpha)\n",
    "            clf.fit(X_train_diff, y_train)\n",
    "            nonzero_features = np.logical_and(nonzero_features, (clf.coef_ > 0))\n",
    "\n",
    "        nonzero_indices = np.where(nonzero_features)[1]\n",
    "        X_train_diff_remain = X_train_diff[:, nonzero_indices]\n",
    "        print(\"Number of nonzero features: {}\".format(nonzero_indices.shape[0]))\n",
    "        coef_sums = np.zeros(X_train_diff_remain.shape[1])\n",
    "\n",
    "        for j in range(n_repeats):\n",
    "            clf = SGDClassifier(loss=\"log\", penalty=\"l2\", alpha=alpha_range[k])\n",
    "            clf.fit(X_train_diff_remain, y_train)\n",
    "            coef_sums = coef_sums + clf.coef_\n",
    "\n",
    "        most_significant = np.argsort(coef_sums)[0, :78]\n",
    "        features_selected = nonzero_indices[most_significant]\n",
    "\n",
    "        X_bl_train = X_train_baseline[:, features_selected]\n",
    "        X_bl_test = X_test_baseline[:, features_selected]\n",
    "        X_vartp_train = X_train_followup[:, features_selected]\n",
    "        X_vartp_test = X_test_followup[:, features_selected]\n",
    "        print(\"Time required : {}\".format(time.time() - tstart))\n",
    "        np.save(ADNI_out + \"RLR_bl_train_{}_cv{}.npy\".format(t,i), X_bl_train)\n",
    "        np.save(ADNI_out + \"RLR_bl_test_{}_cv{}.npy\".format(t,i), X_bl_test)\n",
    "        np.save(ADNI_out + \"RLR_vartp_train_{}_cv{}.npy\".format(t,i), X_vartp_train)\n",
    "        np.save(ADNI_out + \"RLR_vartp_test_{}_cv{}.npy\".format(t,i), X_vartp_test)\n",
    "        \n",
    "        if t == 'MMSE': # Reduce the AIBL replication data\n",
    "            aibl_bl_reduced = aibl_baseline[:, features_selected]\n",
    "            aibl_vartp_reduced = aibl_followup[:, features_selected]\n",
    "            np.save(AIBL_out + \"RLR_bl_cv{}.npy\".format(i), aibl_bl_reduced)\n",
    "            np.save(AIBL_out + \"RLR_vartp_cv{}.npy\".format(i), aibl_vartp_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reduce the data with HCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_features = 78 # same number of components as AAL\n",
    "n_partitions = 20 # 2-step HCA process\n",
    "partition_size = 40962/n_partitions\n",
    "n_clusters = int(partition_size/n_partitions)\n",
    "splits = pd.read_pickle(splits_dir + \"train_test_splits.pkl\")\n",
    "\n",
    "for t in ['MMSE', 'ADAS13']:\n",
    "    for i in range(ncv):\n",
    "        train_split = splits[t][\"train\"][i]\n",
    "        test_split = splits[t][\"test\"][i]\n",
    "        X_train_baseline = X_baseline[train_split]\n",
    "        X_test_baseline = X_baseline[test_split]\n",
    "        X_train_followup = X_followup[train_split]\n",
    "        X_test_followup = X_followup[test_split]\n",
    "        X_train_diff = X_diff[train_split]\n",
    "\n",
    "        tstart = time.time()\n",
    "        print(\"Beginning HCA, {} trajectory, CV fold number {}\".format(t,i))\n",
    "        print(\"Clustering left vertices...\")\n",
    "\n",
    "        left_vertices = X_train_diff.T[:40962, :]\n",
    "        left_vertices_merged = []\n",
    "        left_cluster_list = []\n",
    "\n",
    "        for p in range(n_partitions):\n",
    "            vp = left_vertices[int(p*partition_size):int((p+1)*partition_size)]\n",
    "            hca = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "            clustering = hca.fit(vp)\n",
    "\n",
    "            for cluster in np.unique(clustering.labels_):\n",
    "                ind = np.where(clustering.labels_ == cluster)[0]\n",
    "                left_cluster_list.append(ind + int(p*partition_size))\n",
    "                left_vertices_merged.append(np.mean(vp[ind, :], axis=0))\n",
    "\n",
    "        hca = AgglomerativeClustering(n_clusters=int(np.floor(n_features/2)), linkage=\"ward\")\n",
    "        left_clustering = hca.fit(np.array(left_vertices_merged))\n",
    "\n",
    "        print(\"Clustering right vertices...\")\n",
    "\n",
    "        right_vertices = X_train_diff.T[-40962:, :]\n",
    "        right_vertices_merged = []\n",
    "        right_cluster_list = []\n",
    "\n",
    "        for p in range(n_partitions):\n",
    "            vp = right_vertices[int(p*partition_size):int((p+1)*partition_size)]\n",
    "            hca = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "            clustering = hca.fit(vp)\n",
    "\n",
    "            for cluster in np.unique(clustering.labels_):\n",
    "                ind = np.where(clustering.labels_ == cluster)[0]\n",
    "                right_cluster_list.append(ind + int(p*partition_size))\n",
    "                right_vertices_merged.append(np.mean(vp[ind, :], axis=0))\n",
    "\n",
    "        hca = AgglomerativeClustering(n_clusters=int(np.ceil(n_features/2)), linkage=\"ward\")\n",
    "        right_clustering = hca.fit(np.array(right_vertices_merged))\n",
    "\n",
    "        print(\"Reducing the data...\")\n",
    "\n",
    "        X_bl_train_reduced = np.zeros((X_train_baseline.shape[0], n_features))\n",
    "        X_bl_test_reduced = np.zeros((X_test_baseline.shape[0], n_features))\n",
    "        X_vartp_train_reduced = np.zeros((X_train_followup.shape[0], n_features))\n",
    "        X_vartp_test_reduced = np.zeros((X_test_followup.shape[0], n_features))\n",
    "        \n",
    "        if t == 'MMSE': # Reduce the AIBL replication data\n",
    "            aibl_bl_reduced = np.zeros((aibl_baseline.shape[0], n_features))\n",
    "            aibl_vartp_reduced = np.zeros((aibl_followup.shape[0], n_features))\n",
    "\n",
    "        for cluster in np.unique(left_clustering.labels_):\n",
    "            subclusters = np.where(left_clustering.labels_ == cluster)[0]\n",
    "            ind = np.concatenate([left_cluster_list[sc] for sc in subclusters])\n",
    "            X_bl_train_reduced[:, cluster] = np.mean(X_train_baseline[:, ind], axis=1)\n",
    "            X_bl_test_reduced[:, cluster] = np.mean(X_test_baseline[:, ind], axis=1)\n",
    "            X_vartp_train_reduced[:, cluster] = np.mean(X_train_followup[:, ind], axis=1)\n",
    "            X_vartp_test_reduced[:, cluster] = np.mean(X_test_followup[:, ind], axis=1)\n",
    "            \n",
    "            if t == 'MMSE': \n",
    "                aibl_bl_reduced[:, cluster] = np.mean(aibl_baseline[:, ind], axis=1)\n",
    "                aibl_vartp_reduced[:, cluster] = np.mean(aibl_followup[:, ind], axis=1)\n",
    "\n",
    "        shift = int(np.floor(n_features/2))\n",
    "        for cluster in np.unique(right_clustering.labels_):\n",
    "            subclusters = np.where(right_clustering.labels_ == cluster)[0]\n",
    "            ind = np.concatenate([right_cluster_list[sc] for sc in subclusters])\n",
    "            X_bl_train_reduced[:, cluster + shift] = np.mean(X_train_baseline[:, ind], axis=1)\n",
    "            X_bl_test_reduced[:, cluster + shift] = np.mean(X_test_baseline[:, ind], axis=1)\n",
    "            X_vartp_train_reduced[:, cluster + shift] = np.mean(X_train_followup[:, ind], axis=1)\n",
    "            X_vartp_test_reduced[:, cluster + shift] = np.mean(X_test_followup[:, ind], axis=1)\n",
    "            \n",
    "            if t == 'MMSE': \n",
    "                aibl_bl_reduced[:, cluster + shift] = np.mean(aibl_baseline[:, ind], axis=1)\n",
    "                aibl_vartp_reduced[:, cluster + shift] = np.mean(aibl_followup[:, ind], axis=1)\n",
    "\n",
    "        print(\"Time required : {}\".format(time.time() - tstart))\n",
    "        np.save(ADNI_out + \"HCA_bl_train_{}_cv{}.npy\".format(t,i), X_bl_train_reduced)\n",
    "        np.save(ADNI_out + \"HCA_bl_test_{}_cv{}.npy\".format(t,i), X_bl_test_reduced)\n",
    "        np.save(ADNI_out + \"HCA_vartp_train_{}_cv{}.npy\".format(t,i), X_vartp_train_reduced)\n",
    "        np.save(ADNI_out + \"HCA_vartp_test_{}_cv{}.npy\".format(t,i), X_vartp_test_reduced)\n",
    "        \n",
    "        if t == 'MMSE': # Reduce the AIBL replication data\n",
    "            np.save(AIBL_out + \"HCA_bl_cv{}.npy\".format(i), aibl_bl_reduced)\n",
    "            np.save(AIBL_out + \"HCA_vartp_cv{}.npy\".format(i), aibl_vartp_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
