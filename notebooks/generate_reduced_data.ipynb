{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new data, if needed\n",
    "\n",
    "X_baseline = [] # baseline time point\n",
    "for filename in os.listdir(\"ADNIMERGE/baseline/\"):\n",
    "    ct = np.loadtxt(\"ADNIMERGE/baseline/\"+filename, delimiter='\\n')\n",
    "    X_baseline.append(ct)\n",
    "    \n",
    "X_followup = [] # other longitudinal time point\n",
    "for filename in os.listdir(\"ADNIMERGE/followup/\"):\n",
    "    ct = np.loadtxt(\"ADNIMERGE/followup/\"+filename, delimiter='\\n')\n",
    "    X_followup.append(ct)\n",
    "    \n",
    "np.save(\"X_baseline.npy\", np.array(X_baseline))\n",
    "np.save(\"X_followup.npy\", np.array(X_followup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1061, 81924)\n",
      "(1061, 81924)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "sub_list = pd.read_csv(\"Exp_CL1_sub_list.csv\")\n",
    "df = pd.read_csv(\"Exp_502_602_combined.csv\")\n",
    "X_baseline = np.load(\"X_baseline.npy\")\n",
    "X_followup = np.load(\"X_followup.npy\")\n",
    "X_diff = X_followup - X_baseline # vertex-wise CT change\n",
    "n_subjects = sub_list.shape[0]\n",
    "print(X_baseline.shape)\n",
    "print(X_followup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the train-test splits\n",
    "\n",
    "frac_train = 0.8\n",
    "ncv = 10\n",
    "splits = {\"train\": [], \"test\": []}\n",
    "\n",
    "for i in range(ncv):\n",
    "    indices = list(range(n_subjects))\n",
    "    random.shuffle(indices)\n",
    "    train_split = indices[:int(0.8*len(indices))]\n",
    "    valid_split = indices[int(0.8*len(indices)):]\n",
    "    splits[\"train\"].append(train_split)\n",
    "    splits[\"test\"].append(valid_split)\n",
    "    \n",
    "splits = pd.DataFrame.from_dict(splits)\n",
    "splits.to_pickle(\"train_test_splits.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required : 25.57176661491394\n",
      "Time required : 13.149729251861572\n",
      "Time required : 14.0746169090271\n",
      "Time required : 14.54721474647522\n",
      "Time required : 14.454704284667969\n",
      "Time required : 14.827574253082275\n",
      "Time required : 14.644176959991455\n",
      "Time required : 14.727607488632202\n",
      "Time required : 14.855762481689453\n",
      "Time required : 14.59036636352539\n"
     ]
    }
   ],
   "source": [
    "# Reduce the data with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 78 # same number of components as AAL\n",
    "splits = pd.read_pickle(\"train_test_splits.pkl\")\n",
    "\n",
    "for i in range(ncv):\n",
    "    train_split = splits[\"train\"][i]\n",
    "    test_split = splits[\"test\"][i]\n",
    "    X_train_baseline = X_baseline[train_split]\n",
    "    X_test_baseline = X_baseline[test_split]\n",
    "    X_train_followup = X_followup[train_split]\n",
    "    X_test_followup = X_followup[test_split]\n",
    "    X_train_diff = X_diff[train_split]\n",
    "    \n",
    "    tstart = time.time()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_train_diff)\n",
    "    X_bl_train_reduced = pca.transform(X_train_baseline)\n",
    "    X_bl_test_reduced = pca.transform(X_test_baseline)\n",
    "    X_vartp_train_reduced = pca.transform(X_train_followup)\n",
    "    X_vartp_test_reduced = pca.transform(X_test_followup)\n",
    "    print(\"Time required : {}\".format(time.time() - tstart))\n",
    "    np.save(\"data/PCA_bl_train_cv{}.npy\".format(i), X_bl_train_reduced)\n",
    "    np.save(\"data/PCA_bl_test_cv{}.npy\".format(i), X_bl_test_reduced)\n",
    "    np.save(\"data/PCA_vartp_train_cv{}.npy\".format(i), X_vartp_train_reduced)\n",
    "    np.save(\"data/PCA_vartp_test_cv{}.npy\".format(i), X_vartp_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required : 126.70930600166321\n",
      "Time required : 175.61174726486206\n",
      "Time required : 124.69630217552185\n",
      "Time required : 174.79371762275696\n",
      "Time required : 123.47172927856445\n",
      "Time required : 180.70509338378906\n",
      "Time required : 129.811341047287\n",
      "Time required : 167.86436414718628\n",
      "Time required : 122.67340397834778\n",
      "Time required : 171.08620715141296\n",
      "Time required : 130.93012809753418\n",
      "Time required : 173.38778924942017\n",
      "Time required : 123.8691246509552\n",
      "Time required : 172.9858980178833\n",
      "Time required : 125.03823852539062\n",
      "Time required : 183.5230188369751\n",
      "Time required : 137.060405254364\n",
      "Time required : 168.04962372779846\n",
      "Time required : 122.51985120773315\n",
      "Time required : 168.7333207130432\n"
     ]
    }
   ],
   "source": [
    "# Reduce the data with RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "n_features = 78 # same number of components as AAL\n",
    "splits = pd.read_pickle(\"train_test_splits.pkl\")\n",
    "\n",
    "for i in range(ncv):\n",
    "    train_split = splits[\"train\"][i]\n",
    "    test_split = splits[\"test\"][i]\n",
    "    X_train_baseline = X_baseline[train_split]\n",
    "    X_test_baseline = X_baseline[test_split]\n",
    "    X_train_followup = X_followup[train_split]\n",
    "    X_test_followup = X_followup[test_split]\n",
    "    X_train_diff = X_diff[train_split]\n",
    "    \n",
    "    ptid = sub_list[\"PTID\"]\n",
    "    train_ptid = ptid[train_split]\n",
    "    df_train = df[df[\"PTID\"].isin(train_ptid)]\n",
    "    y_train_mmse = df_train[\"MMSE_2c_traj\"]\n",
    "    y_train_adas = df_train[\"ADAS_3c_traj\"]\n",
    "    \n",
    "    tstart = time.time()\n",
    "    estimator = SVC(kernel=\"linear\")\n",
    "    rfe = RFE(estimator, n_features, step=0.5)\n",
    "    rfe.fit(X_train_diff, y_train_mmse)\n",
    "    X_bl_train_mmse = rfe.transform(X_train_baseline)\n",
    "    X_bl_test_mmse = rfe.transform(X_test_baseline)\n",
    "    X_vartp_train_mmse = rfe.transform(X_train_followup)\n",
    "    X_vartp_test_mmse = rfe.transform(X_test_followup)\n",
    "    print(\"Time required : {}\".format(time.time() - tstart))\n",
    "    np.save(\"data/RFE_bl_train_MMSE_cv{}.npy\".format(i), X_bl_train_mmse)\n",
    "    np.save(\"data/RFE_bl_test_MMSE_cv{}.npy\".format(i), X_bl_test_mmse)\n",
    "    np.save(\"data/RFE_vartp_train_MMSE_cv{}.npy\".format(i), X_vartp_train_mmse)\n",
    "    np.save(\"data/RFE_vartp_test_MMSE_cv{}.npy\".format(i), X_vartp_test_mmse)\n",
    "    \n",
    "    tstart = time.time()\n",
    "    estimator = SVC(kernel=\"linear\")\n",
    "    rfe = RFE(estimator, n_features, step=0.5)\n",
    "    rfe.fit(X_train_diff, y_train_adas)\n",
    "    X_bl_train_adas = rfe.transform(X_train_baseline)\n",
    "    X_bl_test_adas = rfe.transform(X_test_baseline)\n",
    "    X_vartp_train_adas = rfe.transform(X_train_followup)\n",
    "    X_vartp_test_adas = rfe.transform(X_test_followup)\n",
    "    print(\"Time required : {}\".format(time.time() - tstart))\n",
    "    np.save(\"data/RFE_bl_train_ADAS13_cv{}.npy\".format(i), X_bl_train_adas)\n",
    "    np.save(\"data/RFE_bl_test_ADAS13_cv{}.npy\".format(i), X_bl_test_adas)\n",
    "    np.save(\"data/RFE_vartp_train_ADAS13_cv{}.npy\".format(i), X_vartp_train_adas)\n",
    "    np.save(\"data/RFE_vartp_test_ADAS13_cv{}.npy\".format(i), X_vartp_test_adas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of alphas: [1.00000000e-05 2.15443469e-05 4.64158883e-05 1.00000000e-04\n",
      " 2.15443469e-04 4.64158883e-04 1.00000000e-03 2.15443469e-03\n",
      " 4.64158883e-03 1.00000000e-02]\n",
      "Begin MMSE, CV fold number 0\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/quarantine/anaconda/5.1.0-python3/install/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 4.641588833612782e-05\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 897\n",
      "Time required : 251.42656755447388\n",
      "Begin ADAS13, CV fold number 0\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.01\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 3981\n",
      "Time required : 619.5794517993927\n",
      "Begin MMSE, CV fold number 1\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.002154434690031882\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 767\n",
      "Time required : 223.5866858959198\n",
      "Begin ADAS13, CV fold number 1\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.00021544346900318823\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 2406\n",
      "Time required : 691.8239126205444\n",
      "Begin MMSE, CV fold number 2\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.002154434690031882\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 784\n",
      "Time required : 223.30160093307495\n",
      "Begin ADAS13, CV fold number 2\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.004641588833612777\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 2656\n",
      "Time required : 682.233068227768\n",
      "Begin MMSE, CV fold number 3\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.001\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 803\n",
      "Time required : 227.89899039268494\n",
      "Begin ADAS13, CV fold number 3\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.00046415888336127773\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 2937\n",
      "Time required : 681.4507184028625\n",
      "Begin MMSE, CV fold number 4\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 2.1544346900318823e-05\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 1081\n",
      "Time required : 247.4402093887329\n",
      "Begin ADAS13, CV fold number 4\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.0001\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 2754\n",
      "Time required : 701.4533424377441\n",
      "Begin MMSE, CV fold number 5\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.002154434690031882\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 815\n",
      "Time required : 246.7669219970703\n",
      "Begin ADAS13, CV fold number 5\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.01\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 3649\n",
      "Time required : 631.3030366897583\n",
      "Begin MMSE, CV fold number 6\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 4.641588833612782e-05\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 897\n",
      "Time required : 252.67927265167236\n",
      "Begin ADAS13, CV fold number 6\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.001\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 2494\n",
      "Time required : 669.184873342514\n",
      "Begin MMSE, CV fold number 7\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.0001\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 863\n",
      "Time required : 254.84995889663696\n",
      "Begin ADAS13, CV fold number 7\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.0001\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 2760\n",
      "Time required : 714.9980502128601\n",
      "Begin MMSE, CV fold number 8\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.002154434690031882\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 1436\n",
      "Time required : 220.98642301559448\n",
      "Begin ADAS13, CV fold number 8\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.002154434690031882\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 2565\n",
      "Time required : 647.2577908039093\n",
      "Begin MMSE, CV fold number 9\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.00021544346900318823\n",
      "Step two: find the most significant features\n",
      "Number of nonzero features: 1046\n",
      "Time required : 237.39214372634888\n",
      "Begin ADAS13, CV fold number 9\n",
      "Step one: choose the optimal alpha\n",
      "Beginning repeat 0...\n",
      "Beginning repeat 1...\n",
      "Beginning repeat 2...\n",
      "Beginning repeat 3...\n",
      "Beginning repeat 4...\n",
      "Optimal alpha: 0.01\n",
      "Step two: find the nonzero features\n",
      "Number of nonzero features: 4131\n",
      "Time required : 619.2317228317261\n"
     ]
    }
   ],
   "source": [
    "# Reduce the data with RLR\n",
    "# Idea by Moradi et al., 2015\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "n_features = 78 # same number of components as AAL\n",
    "splits = pd.read_pickle(\"train_test_splits.pkl\")\n",
    "\n",
    "n_repeats = 5 # 10 by Moradi et al.\n",
    "range_size = 10 # 100 by Moradi et al.\n",
    "alpha_range = np.power(10, np.linspace(-5, -2, range_size))\n",
    "print(\"Range of alphas: {}\".format(alpha_range))\n",
    "\n",
    "for i in range(ncv):\n",
    "    train_split = splits[\"train\"][i]\n",
    "    test_split = splits[\"test\"][i]\n",
    "    X_train_baseline = X_baseline[train_split]\n",
    "    X_test_baseline = X_baseline[test_split]\n",
    "    X_train_followup = X_followup[train_split]\n",
    "    X_test_followup = X_followup[test_split]\n",
    "    X_train_diff = X_diff[train_split]\n",
    "    \n",
    "    ptid = sub_list[\"PTID\"]\n",
    "    train_ptid = ptid[train_split]\n",
    "    df_train = df[df[\"PTID\"].isin(train_ptid)]\n",
    "    y_train_mmse = df_train[\"MMSE_2c_traj\"]\n",
    "    y_train_adas = df_train[\"ADAS_3c_traj\"]\n",
    "    \n",
    "    print(\"Begin MMSE, CV fold number {}\".format(i))\n",
    "    print(\"Step one: choose the optimal alpha\")\n",
    "    tstart = time.time()\n",
    "    alpha_star = []\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        print(\"Beginning repeat {}...\".format(j))\n",
    "        scores = np.zeros(range_size)\n",
    "        \n",
    "        indices = list(range(len(train_split)))\n",
    "        random.shuffle(indices)\n",
    "        inner_train = indices[:int(0.9*len(indices))]\n",
    "        inner_valid = indices[int(0.9*len(indices)):]\n",
    "        X_diff_inner_train = X_diff[inner_train]\n",
    "        X_diff_inner_valid = X_diff[inner_valid]\n",
    "        inner_train_ptid = ptid[inner_train]\n",
    "        inner_valid_ptid = ptid[inner_valid]\n",
    "        df_inner_train = df[df[\"PTID\"].isin(inner_train_ptid)]\n",
    "        df_inner_valid = df[df[\"PTID\"].isin(inner_valid_ptid)]\n",
    "        y_inner_train_mmse = df_inner_train[\"MMSE_2c_traj\"]\n",
    "        y_inner_valid_mmse = df_inner_valid[\"MMSE_2c_traj\"]\n",
    "        \n",
    "        for k in range(range_size):\n",
    "            clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", l1_ratio=0.5, alpha=alpha_range[k])\n",
    "            clf.fit(X_diff_inner_train, y_inner_train_mmse)\n",
    "            scores[k] = clf.score(X_diff_inner_valid, y_inner_valid_mmse)\n",
    "        \n",
    "        alpha_star.append(alpha_range[np.argmax(scores)])\n",
    "        \n",
    "    print(\"Optimal alpha: {}\".format(np.median(alpha_star)))\n",
    "    print(\"Step two: find the most significant features\")\n",
    "    k = np.where(alpha_range <= np.median(alpha_star))[0][-1]\n",
    "    nonzero_features = np.ones(X_baseline.shape[1])\n",
    "    \n",
    "    for alpha in alpha_range[k:]:\n",
    "        clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\",\n",
    "                            l1_ratio=0.5, alpha=alpha)\n",
    "        clf.fit(X_train_diff, y_train_mmse)\n",
    "        nonzero_features = np.logical_and(nonzero_features, (clf.coef_ > 0))\n",
    "    \n",
    "    nonzero_indices = np.where(nonzero_features)[1]\n",
    "    X_train_diff_remain = X_train_diff[:, nonzero_indices]\n",
    "    print(\"Number of nonzero features: {}\".format(nonzero_indices.shape[0]))\n",
    "    coef_sums = np.zeros(X_train_diff_remain.shape[1])\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        clf = SGDClassifier(loss=\"log\", penalty=\"l2\", alpha=alpha_range[k])\n",
    "        clf.fit(X_train_diff_remain, y_train_mmse)\n",
    "        coef_sums = coef_sums + clf.coef_\n",
    "        \n",
    "    most_significant = np.argsort(coef_sums)[0, :78]\n",
    "    features_selected = nonzero_indices[most_significant]\n",
    "    \n",
    "    X_bl_train_mmse = X_train_baseline[:, features_selected]\n",
    "    X_bl_test_mmse = X_test_baseline[:, features_selected]\n",
    "    X_vartp_train_mmse = X_train_followup[:, features_selected]\n",
    "    X_vartp_test_mmse = X_test_followup[:, features_selected]\n",
    "    print(\"Time required : {}\".format(time.time() - tstart))\n",
    "    np.save(\"data/RLR_bl_train_MMSE_cv{}.npy\".format(i), X_bl_train_mmse)\n",
    "    np.save(\"data/RLR_bl_test_MMSE_cv{}.npy\".format(i), X_bl_test_mmse)\n",
    "    np.save(\"data/RLR_vartp_train_MMSE_cv{}.npy\".format(i), X_vartp_train_mmse)\n",
    "    np.save(\"data/RLR_vartp_test_MMSE_cv{}.npy\".format(i), X_vartp_test_mmse)\n",
    "    \n",
    "    \n",
    "    print(\"Begin ADAS13, CV fold number {}\".format(i))\n",
    "    print(\"Step one: choose the optimal alpha\")\n",
    "    tstart = time.time()\n",
    "    alpha_star = []\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        print(\"Beginning repeat {}...\".format(j))\n",
    "        scores = np.zeros(range_size)\n",
    "        \n",
    "        indices = list(range(len(train_split)))\n",
    "        random.shuffle(indices)\n",
    "        inner_train = indices[:int(0.9*len(indices))]\n",
    "        inner_valid = indices[int(0.9*len(indices)):]\n",
    "        X_diff_inner_train = X_diff[inner_train]\n",
    "        X_diff_inner_valid = X_diff[inner_valid]\n",
    "        inner_train_ptid = ptid[inner_train]\n",
    "        inner_valid_ptid = ptid[inner_valid]\n",
    "        df_inner_train = df[df[\"PTID\"].isin(inner_train_ptid)]\n",
    "        df_inner_valid = df[df[\"PTID\"].isin(inner_valid_ptid)]\n",
    "        y_inner_train_adas = df_inner_train[\"ADAS_3c_traj\"]\n",
    "        y_inner_valid_adas = df_inner_valid[\"ADAS_3c_traj\"]\n",
    "        \n",
    "        for k in range(range_size):\n",
    "            clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", l1_ratio=0.5, alpha=alpha_range[k])\n",
    "            clf.fit(X_diff_inner_train, y_inner_train_adas)\n",
    "            scores[k] = clf.score(X_diff_inner_valid, y_inner_valid_adas)\n",
    "        \n",
    "        alpha_star.append(alpha_range[np.argmax(scores)])\n",
    "        \n",
    "    print(\"Optimal alpha: {}\".format(np.median(alpha_star)))\n",
    "    print(\"Step two: find the nonzero features\")\n",
    "    k = np.where(alpha_range <= np.median(alpha_star))[0][-1]\n",
    "    nonzero_features = np.ones(X_baseline.shape[1])\n",
    "    \n",
    "    for alpha in alpha_range[k:]:\n",
    "        clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\",\n",
    "                            l1_ratio=0.5, alpha=alpha)\n",
    "        clf.fit(X_train_diff, y_train_adas)\n",
    "        nonzero_features = np.logical_and(nonzero_features, (clf.coef_ > 0))\n",
    "    \n",
    "    nonzero_indices = np.where(nonzero_features)[1]\n",
    "    X_train_diff_remain = X_train_diff[:, nonzero_indices]\n",
    "    print(\"Number of nonzero features: {}\".format(nonzero_indices.shape[0]))\n",
    "    coef_sums = np.zeros(X_train_diff_remain.shape[1])\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        clf = SGDClassifier(loss=\"log\", penalty=\"l2\", alpha=alpha_range[k])\n",
    "        clf.fit(X_train_diff_remain, y_train_adas)\n",
    "        coef_sums = coef_sums + clf.coef_\n",
    "        \n",
    "    most_significant = np.argsort(coef_sums)[0, :78]\n",
    "    features_selected = nonzero_indices[most_significant]\n",
    "    \n",
    "    X_bl_train_adas = X_train_baseline[:, features_selected]\n",
    "    X_bl_test_adas = X_test_baseline[:, features_selected]\n",
    "    X_vartp_train_adas = X_train_followup[:, features_selected]\n",
    "    X_vartp_test_adas = X_test_followup[:, features_selected]\n",
    "    print(\"Time required : {}\".format(time.time() - tstart))\n",
    "    np.save(\"data/RLR_bl_train_ADAS13_cv{}.npy\".format(i), X_bl_train_adas)\n",
    "    np.save(\"data/RLR_bl_test_ADAS13_cv{}.npy\".format(i), X_bl_test_adas)\n",
    "    np.save(\"data/RLR_vartp_train_ADAS13_cv{}.npy\".format(i), X_vartp_train_adas)\n",
    "    np.save(\"data/RLR_vartp_test_ADAS13_cv{}.npy\".format(i), X_vartp_test_adas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning HCA, CV fold number 0\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 62.664196252822876\n",
      "Beginning HCA, CV fold number 1\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 62.65158700942993\n",
      "Beginning HCA, CV fold number 2\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 64.05795669555664\n",
      "Beginning HCA, CV fold number 3\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 62.70123505592346\n",
      "Beginning HCA, CV fold number 4\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 64.898353099823\n",
      "Beginning HCA, CV fold number 5\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 64.6041955947876\n",
      "Beginning HCA, CV fold number 6\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 63.10928773880005\n",
      "Beginning HCA, CV fold number 7\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 65.51651549339294\n",
      "Beginning HCA, CV fold number 8\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 64.07316184043884\n",
      "Beginning HCA, CV fold number 9\n",
      "Clustering left vertices...\n",
      "Clustering right vertices...\n",
      "Reducing the data...\n",
      "Time required : 63.73818826675415\n"
     ]
    }
   ],
   "source": [
    "# Reduce the data with HCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_features = 78 # same number of components as AAL\n",
    "n_partitions = 20 # 2-step HCA process\n",
    "partition_size = 40962/n_partitions\n",
    "n_clusters = int(partition_size/n_partitions)\n",
    "splits = pd.read_pickle(\"train_test_splits.pkl\")\n",
    "\n",
    "for i in range(ncv):\n",
    "    train_split = splits[\"train\"][i]\n",
    "    test_split = splits[\"test\"][i]\n",
    "    X_train_baseline = X_baseline[train_split]\n",
    "    X_test_baseline = X_baseline[test_split]\n",
    "    X_train_followup = X_followup[train_split]\n",
    "    X_test_followup = X_followup[test_split]\n",
    "    X_train_diff = X_diff[train_split]\n",
    "    \n",
    "    def generate_new_features(X, left, right):\n",
    "        new_features = np.zeros((X.shape[0], n_features))\n",
    "        \n",
    "        for cluster in np.unique(left_clustering.labels_):\n",
    "            vertices = X[:, np.where(left_clustering.labels_ == cluster)[0]]\n",
    "            new_features[:, cluster] = np.mean(vertices)\n",
    "            \n",
    "        for cluster in np.unique(right_clustering.labels_):\n",
    "            vertices = X[:, np.where(right_clustering.labels_ == cluster)[0]]\n",
    "            new_features[:, cluster + 39] = np.mean(vertices)\n",
    "        \n",
    "        return new_features\n",
    "    \n",
    "    tstart = time.time()\n",
    "    print(\"Beginning HCA, CV fold number {}\".format(i))\n",
    "    print(\"Clustering left vertices...\")\n",
    "    \n",
    "    left_vertices = X_train_diff.T[:40962, :]\n",
    "    left_vertices_merged = []\n",
    "    left_cluster_list = []\n",
    "    \n",
    "    for p in range(n_partitions):\n",
    "        vp = left_vertices[int(p*partition_size):int((p+1)*partition_size)]\n",
    "        hca = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "        clustering = hca.fit(vp)\n",
    "        \n",
    "        for cluster in np.unique(clustering.labels_):\n",
    "            ind = np.where(clustering.labels_ == cluster)[0]\n",
    "            left_cluster_list.append(ind + int(p*partition_size))\n",
    "            left_vertices_merged.append(np.mean(vp[ind, :], axis=0))\n",
    "\n",
    "    hca = AgglomerativeClustering(n_clusters=int(np.floor(n_features/2)), linkage=\"ward\")\n",
    "    left_clustering = hca.fit(np.array(left_vertices_merged))\n",
    "        \n",
    "    print(\"Clustering right vertices...\")\n",
    "    \n",
    "    right_vertices = X_train_diff.T[-40962:, :]\n",
    "    right_vertices_merged = []\n",
    "    right_cluster_list = []\n",
    "    \n",
    "    for p in range(n_partitions):\n",
    "        vp = right_vertices[int(p*partition_size):int((p+1)*partition_size)]\n",
    "        hca = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "        clustering = hca.fit(vp)\n",
    "        \n",
    "        for cluster in np.unique(clustering.labels_):\n",
    "            ind = np.where(clustering.labels_ == cluster)[0]\n",
    "            right_cluster_list.append(ind + int(p*partition_size))\n",
    "            right_vertices_merged.append(np.mean(vp[ind, :], axis=0))\n",
    "            \n",
    "    hca = AgglomerativeClustering(n_clusters=int(np.ceil(n_features/2)), linkage=\"ward\")\n",
    "    right_clustering = hca.fit(np.array(right_vertices_merged))\n",
    "    \n",
    "    print(\"Reducing the data...\")\n",
    "    \n",
    "    X_bl_train_reduced = np.zeros((X_train_baseline.shape[0], n_features))\n",
    "    X_bl_test_reduced = np.zeros((X_test_baseline.shape[0], n_features))\n",
    "    X_vartp_train_reduced = np.zeros((X_train_followup.shape[0], n_features))\n",
    "    X_vartp_test_reduced = np.zeros((X_test_followup.shape[0], n_features))\n",
    "    \n",
    "    for cluster in np.unique(left_clustering.labels_):\n",
    "        subclusters = np.where(left_clustering.labels_ == cluster)[0]\n",
    "        ind = np.concatenate([left_cluster_list[sc] for sc in subclusters])\n",
    "        X_bl_train_reduced[:, cluster] = np.mean(X_train_baseline[:, ind], axis=1)\n",
    "        X_bl_test_reduced[:, cluster] = np.mean(X_test_baseline[:, ind], axis=1)\n",
    "        X_vartp_train_reduced[:, cluster] = np.mean(X_train_followup[:, ind], axis=1)\n",
    "        X_vartp_test_reduced[:, cluster] = np.mean(X_test_followup[:, ind], axis=1)\n",
    "        \n",
    "    shift = int(np.floor(n_features/2))\n",
    "    for cluster in np.unique(right_clustering.labels_):\n",
    "        subclusters = np.where(right_clustering.labels_ == cluster)[0]\n",
    "        ind = np.concatenate([right_cluster_list[sc] for sc in subclusters])\n",
    "        X_bl_train_reduced[:, cluster + shift] = np.mean(X_train_baseline[:, ind], axis=1)\n",
    "        X_bl_test_reduced[:, cluster + shift] = np.mean(X_test_baseline[:, ind], axis=1)\n",
    "        X_vartp_train_reduced[:, cluster + shift] = np.mean(X_train_followup[:, ind], axis=1)\n",
    "        X_vartp_test_reduced[:, cluster + shift] = np.mean(X_test_followup[:, ind], axis=1)\n",
    "    \n",
    "    print(\"Time required : {}\".format(time.time() - tstart))\n",
    "    np.save(\"data/HCA_bl_train_cv{}.npy\".format(i), X_bl_train_reduced)\n",
    "    np.save(\"data/HCA_bl_test_cv{}.npy\".format(i), X_bl_test_reduced)\n",
    "    np.save(\"data/HCA_vartp_train_cv{}.npy\".format(i), X_vartp_train_reduced)\n",
    "    np.save(\"data/HCA_vartp_test_cv{}.npy\".format(i), X_vartp_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
